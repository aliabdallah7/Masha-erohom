{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10065 entries, 0 to 10064\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   ID      10065 non-null  int64 \n",
      " 1    TWEET  10064 non-null  object\n",
      " 2    LABEL  10065 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 236.0+ KB\n",
      "None\n",
      "\n",
      "First few rows of the dataset:\n",
      "   ID                                              TWEET    LABEL\n",
      "0   1            Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ø§Ù„Ø¬Ø§ÙŠÙ‡ Ù‡ÙƒÙˆÙ† Ù„Ø³Ù‡ Ù Ø§Ù„ÙƒÙ„ÙŠÙ‡ ..     none\n",
      "1   2  Ø¹Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ù‡ ÙˆØµÙ„ Ù„93.7 % Ù…Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙŠØ¹Ù†ÙŠ...    anger\n",
      "2   3                         ÙƒØªÙ†Ø§ Ù†ÙŠÙ„Ù‡ Ù Ø­Ø¸Ù†Ø§ Ø§Ù„Ù‡Ø¨Ø§Ø¨ xD  sadness\n",
      "3   4  Ø¬Ù…ÙŠØ¹Ù†Ø§ Ù†Ø±ÙŠØ¯ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù‡Ø¯Ø§ÙÙ†Ø§ Ù„ÙƒÙ† ØªÙˆÙ†Ø³ ØªØ§Ù„Ù‚Øª ÙÙŠ Ø­Ø±...      joy\n",
      "4   5  Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ù†Ø¸Ø§Ù…Ù‡Ø§ Ù…Ø®ØªÙ„Ù .. ÙˆÙ…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù…ÙˆÙ†Ø¯ÙŠØ§Ù„ ...     none\n",
      "\n",
      "Missing values:\n",
      "ID        0\n",
      " TWEET    1\n",
      " LABEL    0\n",
      "dtype: int64\n",
      "##################\n",
      "ID        0\n",
      " TWEET    0\n",
      " LABEL    0\n",
      "dtype: int64\n",
      "Index(['ID', ' TWEET', ' LABEL'], dtype='object')\n",
      "Index(['ID', 'TWEET', 'LABEL'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset (assuming it's in CSV format)\n",
    "df = pd.read_csv('Emotional-Tone-Dataset.csv')\n",
    "\n",
    "# Explore the dataset\n",
    "print(\"Dataset information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove null values\n",
    "df = df.dropna()\n",
    "print(\"##################\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Rename Columns\n",
    "print(df.columns)\n",
    "df.rename(columns={' ID': 'ID', ' TWEET': 'TWEET', ' LABEL': 'LABEL'}, inplace=True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TWEET</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ø§Ù„Ø¬Ø§ÙŠÙ‡ Ù‡ÙƒÙˆÙ† Ù„Ø³Ù‡ Ù Ø§Ù„ÙƒÙ„ÙŠÙ‡ ..</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ø¹Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ù‡ ÙˆØµÙ„ Ù„93.7 % Ù…Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙŠØ¹Ù†ÙŠ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ÙƒØªÙ†Ø§ Ù†ÙŠÙ„Ù‡ Ù Ø­Ø¸Ù†Ø§ Ø§Ù„Ù‡Ø¨Ø§Ø¨ xD</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Ø¬Ù…ÙŠØ¹Ù†Ø§ Ù†Ø±ÙŠØ¯ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù‡Ø¯Ø§ÙÙ†Ø§ Ù„ÙƒÙ† ØªÙˆÙ†Ø³ ØªØ§Ù„Ù‚Øª ÙÙŠ Ø­Ø±...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ù†Ø¸Ø§Ù…Ù‡Ø§ Ù…Ø®ØªÙ„Ù .. ÙˆÙ…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù…ÙˆÙ†Ø¯ÙŠØ§Ù„ ...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10060</th>\n",
       "      <td>10061</td>\n",
       "      <td>2222: ÙŠÙ„Ø§ ÙŠØ§ Ø¬Ù…Ø§Ø¹Ù‡ Ø­ÙÙ„Ù‡ Ø¹Ù…Ø±Ùˆ Ø¯ÙŠØ§Ø¨ Ø®Ù„ØµØª Ù†Ø±ÙŠØ­ Ø´Ùˆ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10061</th>\n",
       "      <td>10062</td>\n",
       "      <td>Mohamed5: Ø§ÙŠÙŠÙŠÙŠÙ‡ Ø¯Ø§Ø§Ø§ ğŸ˜²ğŸ˜² Ø§ÙˆØ²ÙŠÙŠÙŠÙ„â¤</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10062</th>\n",
       "      <td>10063</td>\n",
       "      <td>Ø¹Ù…Ù„ØªÙ„Ù‡Ø§ Ø±ÙŠØªÙˆÙŠØª Ø¨Ù…Ù†Ø§Ø³Ø¨Ù‡ Ø³Ø§Ø±Ù‡ Ø¨ØªØ§Ø¹Ù‡ Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ ğŸ˜ƒ</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10063</th>\n",
       "      <td>10064</td>\n",
       "      <td>ÙˆØ¹Ù„ÙŠÙƒ Ù‚Ø¨Ù„Ù†Ø§ ÙŠØ§Ù†Ø¬Ù… Ø§Ù„Ù†Ø¬ÙˆÙ… ÙŠØ§Ø¹Ù†Ø¯Ù„ÙŠØ¨ Ø§Ù„Ø­Ø¨ ÙˆØ§Ù„Ø§Ø­Ø³Ø§Ø³</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10064</th>\n",
       "      <td>10065</td>\n",
       "      <td>AlHamad ÙŠØ·Ù„Ø¹ Ù†Ù†Ù‡Ù… ÙƒÙ„ Ø´ÙŠ Ø³ÙŠØ¡ ÙˆÙˆØ¶ÙŠØ¹ ÙƒÙ„ Ø®Ø³Ø§Ø³Ù‡ Ø§Ù„Ø¹...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10064 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                              TWEET     LABEL\n",
       "0          1            Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ø§Ù„Ø¬Ø§ÙŠÙ‡ Ù‡ÙƒÙˆÙ† Ù„Ø³Ù‡ Ù Ø§Ù„ÙƒÙ„ÙŠÙ‡ ..      none\n",
       "1          2  Ø¹Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ù‡ ÙˆØµÙ„ Ù„93.7 % Ù…Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙŠØ¹Ù†ÙŠ...     anger\n",
       "2          3                         ÙƒØªÙ†Ø§ Ù†ÙŠÙ„Ù‡ Ù Ø­Ø¸Ù†Ø§ Ø§Ù„Ù‡Ø¨Ø§Ø¨ xD   sadness\n",
       "3          4  Ø¬Ù…ÙŠØ¹Ù†Ø§ Ù†Ø±ÙŠØ¯ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù‡Ø¯Ø§ÙÙ†Ø§ Ù„ÙƒÙ† ØªÙˆÙ†Ø³ ØªØ§Ù„Ù‚Øª ÙÙŠ Ø­Ø±...       joy\n",
       "4          5  Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ù†Ø¸Ø§Ù…Ù‡Ø§ Ù…Ø®ØªÙ„Ù .. ÙˆÙ…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù…ÙˆÙ†Ø¯ÙŠØ§Ù„ ...      none\n",
       "...      ...                                                ...       ...\n",
       "10060  10061  2222: ÙŠÙ„Ø§ ÙŠØ§ Ø¬Ù…Ø§Ø¹Ù‡ Ø­ÙÙ„Ù‡ Ø¹Ù…Ø±Ùˆ Ø¯ÙŠØ§Ø¨ Ø®Ù„ØµØª Ù†Ø±ÙŠØ­ Ø´Ùˆ...   sadness\n",
       "10061  10062                  Mohamed5: Ø§ÙŠÙŠÙŠÙŠÙ‡ Ø¯Ø§Ø§Ø§ ğŸ˜²ğŸ˜² Ø§ÙˆØ²ÙŠÙŠÙŠÙ„â¤  surprise\n",
       "10062  10063    Ø¹Ù…Ù„ØªÙ„Ù‡Ø§ Ø±ÙŠØªÙˆÙŠØª Ø¨Ù…Ù†Ø§Ø³Ø¨Ù‡ Ø³Ø§Ø±Ù‡ Ø¨ØªØ§Ø¹Ù‡ Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ ğŸ˜ƒ      none\n",
       "10063  10064    ÙˆØ¹Ù„ÙŠÙƒ Ù‚Ø¨Ù„Ù†Ø§ ÙŠØ§Ù†Ø¬Ù… Ø§Ù„Ù†Ø¬ÙˆÙ… ÙŠØ§Ø¹Ù†Ø¯Ù„ÙŠØ¨ Ø§Ù„Ø­Ø¨ ÙˆØ§Ù„Ø§Ø­Ø³Ø§Ø³       joy\n",
       "10064  10065  AlHamad ÙŠØ·Ù„Ø¹ Ù†Ù†Ù‡Ù… ÙƒÙ„ Ø´ÙŠ Ø³ÙŠØ¡ ÙˆÙˆØ¶ÙŠØ¹ ÙƒÙ„ Ø®Ø³Ø§Ø³Ù‡ Ø§Ù„Ø¹...     anger\n",
       "\n",
       "[10064 rows x 3 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from stopwords import stop_words_ar\n",
    "\n",
    "############### initial variables and lists ##################\n",
    "\n",
    "arabic_punctuations = '''`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:\"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             Ù‘    | # Tashdid\n",
    "                             Ù    | # Fatha\n",
    "                             Ù‹    | # Tanwin Fath\n",
    "                             Ù    | # Damma\n",
    "                             ÙŒ    | # Tanwin Damm\n",
    "                             Ù    | # Kasra\n",
    "                             Ù    | # Tanwin Kasr\n",
    "                             Ù’    | # Sukun\n",
    "                             Ù€     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "arabic_handles = \\\n",
    "    [\n",
    "        ('Ù‡', ['Ø©']),\\\n",
    "        ('ÙŠ', ['Ù‰']),\\\n",
    "        ('Ø§', ['Ø£','Ø¥','Ø¢']),\\\n",
    "        # ('Ùˆ', ['Ø¤']),\\\n",
    "        ('', ['Ù‘','â€˜','ÙŒ','Ù','Ù‹','Ù','Ù','Ù','Ù€','â€™','Ù’','~'])\n",
    "    ]\n",
    "\n",
    "############### functions ##################\n",
    "def clean_text(text):\n",
    "    \"\"\" \n",
    "    It includes these functions:\n",
    "        1-remove_emails\n",
    "        2-remove_URLs\n",
    "        3-remove_mentions\n",
    "        4-hashtags_to_words\n",
    "        5-remove_punctuationsb\n",
    "        6-normalize_arabic\n",
    "        7-remove_diacritics\n",
    "        8-remove_repeating_char\n",
    "        9- remove newlines\n",
    "        10-remove_stop_words\n",
    "        11-remove_emojis\n",
    "        12-remove_english_characters\n",
    "        13-remove_digits\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return ''  # Return an empty string for non-string values\n",
    "    \n",
    "    text=remove_emails(text)\n",
    "    text=remove_URLs(text)\n",
    "    text=remove_mentions(text)\n",
    "    text= hashtags_to_words(text)\n",
    "    text=remove_punctuations(text)\n",
    "    text= remove_newlines(text)\n",
    "    text=normalize_arabic(text)\n",
    "    text=remove_diacritics(text)\n",
    "    text=remove_stop_words(text)\n",
    "    text=replace_emojis(text)\n",
    "    text=remove_english_characters(text)\n",
    "    text=remove_digits(text)\n",
    "    #text=remove_repeating_char(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    \"\"\" normalize the arabic character  .\"\"\"\n",
    "    text = re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
    "    text = re.sub(\"Ú¯\", \"Ùƒ\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_digits(word_list):\n",
    "    # Remove digits\n",
    "    filtered_words = [w for w in word_list if not w.isdigit()]\n",
    "    return filtered_words\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    \"\"\" remove the `arabic diacritics` from the `text` .\"\"\"\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    \"\"\" remove the `punctuations` from the `text` .\"\"\"\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    \"\"\" remove the `repeating character` from the `text` .\"\"\"\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "def remove_newlines(text):\n",
    "  text = re.sub('\\n',\" . \",text)\n",
    "  return text\n",
    "\n",
    "def read_stop_words():\n",
    "    \"\"\" read the `stopwords` \"\"\"\n",
    "    stop_words = stop_words_ar.split('\\n')\n",
    "    #unify arabic letters\n",
    "    for key, arr in arabic_handles:\n",
    "        for a in arr:\n",
    "            stop_words = [word.replace(a, key) for word in stop_words] \n",
    "    return stop_words\n",
    "\n",
    "def remove_english_characters(text):\n",
    "    # Define a regular expression pattern to match English characters\n",
    "    english_pattern = re.compile(\"[a-zA-Z]\")\n",
    "\n",
    "    # Use sub to replace English characters with an empty string\n",
    "    cleaned_text = english_pattern.sub('', text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    \"\"\" remove the `list of Arabic stopwords` from the `text` .\"\"\"\n",
    "    stop_words = read_stop_words()\n",
    "    lines = text.splitlines()\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        if line.strip():  #if not empty\n",
    "            words = line.split()\n",
    "            for w in range(0, len(words)):\n",
    "                word = words[w]\n",
    "                if word in stop_words:\n",
    "                    words[w] = \"\"\n",
    "            line = \" \".join(words)\n",
    "            line = line.replace(\"  \",\" \")\n",
    "            new_lines.append(line)\n",
    "    return '\\n'.join(new_lines)\n",
    "\n",
    "def remove_URLs(text): \n",
    "    \"\"\" remove the `URLs` from the `text` .\"\"\"\n",
    "    text =re.sub(r\"(?:http?\\://|https?\\://|www)\\S+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_emails(text):\n",
    "    \"\"\" remove the `emails` from the `text` .\"\"\"\n",
    "    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+',\"\",text)\n",
    "    return text\n",
    "\n",
    "def remove_mentions(text):\n",
    "    \"\"\" remove the `mentions` from the `text` .\"\"\"\n",
    "    text = re.sub(r\"@([A-Za-z0-9_-]+)\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\" remove the `hashtags` from the `text` .\"\"\"\n",
    "    text = re.sub(r'#\\w+ ?', \"\", text)\n",
    "    return text\n",
    "\n",
    "def hashtags_to_words(text):\n",
    "    \"\"\" convert any `hashtags` to  `words` .\"\"\"\n",
    "    text = re.sub(r'#', \"\", text)\n",
    "    text = re.sub(r\"_\", \"  \", text)\n",
    "    return text\n",
    "\n",
    "def replace_emojis(text): \n",
    "    text = re.sub(r'<3|< 3|â¤ï¸|ğŸ’–|ğŸ˜|ğŸ’•|ğŸ˜˜|ğŸ¥°|ğŸ’•|ğŸ’|ğŸ’—|ğŸ’œ|ğŸ’™|ğŸ–¤|ğŸ’š|ğŸ’›|ğŸ¤|â¤',\n",
    "                  ' Ù‚Ù„Ø¨ ',\n",
    "                  text) \n",
    "\n",
    "    text = re.sub(r':P|:-P|ğŸ˜‚|ğŸ¤£',\n",
    "                  ' Ø¶Ø­Ùƒ ',\n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'[â˜ºğŸ˜ŒğŸ˜ğŸ˜ƒğŸ˜„ğŸ˜†ğŸ˜ŠğŸ˜¸ğŸ˜ºğŸ˜ŠğŸ˜€ğŸ˜‹â˜ºï¸ğŸ™‚ğŸ’ƒ]',\n",
    "                  ' Ø³Ø¹Ø§Ø¯Ø© ',\n",
    "                  text)\n",
    "    \n",
    "    \n",
    "    text = re.sub(r':D',\n",
    "                  ' Ø³Ø¹Ø§Ø¯Ø© ',\n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'[ğŸ˜¥ğŸ˜£ğŸ˜“ğŸ˜”ğŸ˜•â˜¹ï¸ğŸ™ğŸ˜–ğŸ˜ğŸ˜ŸğŸ˜¢ğŸ˜­ğŸ˜©ğŸ˜¿ğŸ˜«ğŸ˜©ğŸ’”]',\n",
    "                  ' Ø­Ø²Ù†  ',\n",
    "                  text)\n",
    "    text = re.sub(r'(::|\\)-:)',\n",
    "                  '  Ø­Ø²Ù†  ', \n",
    "                  text)\n",
    "    text = re.sub(r'(:,\\(|:\\'\\(|:\"\\()',\n",
    "                  ' Ø­Ø²Ù† ', \n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'[ğŸ˜¨ğŸ˜±ğŸ˜µ]',\n",
    "                  ' Ù…ÙØ§Ø¬Ø£Ø© ', \n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'[ğŸ˜³ğŸ˜…ğŸ™ˆ]',\n",
    "                  ' Ù…Ø­Ø±Ø¬ ', \n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'[ğŸ˜¤ğŸ˜ ğŸ˜¡ğŸ¤¬ğŸ‘¿]',\n",
    "                  ' ØºØ¶Ø¨ ', \n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'[ğŸ˜‘ğŸ˜’ğŸ™„ğŸ˜ğŸ˜¶]',\n",
    "                  ' Ù…Ù„Ù„ ', \n",
    "                  text)\n",
    "\n",
    "    text = re.sub('[\\U0001F600-\\U0001FFFF]',\" \", text)\n",
    "    text = re.sub('[\\U0001F300-\\U0001F5FF]',\" \", text)\n",
    "    text = re.sub('[\\U0001F680-\\U0001F6FF]',\" \", text)\n",
    "    text = re.sub('[\"\\U0001F1E0-\\U0001F1FF]',\" \", text)\n",
    "\n",
    "    \n",
    "    weirdPatterns = re.compile(\"[\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u'\\U00010000-\\U0010ffff'\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               u\"\\u2069\"\n",
    "                               u\"\\u2066\"\n",
    "                               u\"\\u200c\"\n",
    "                               u\"\\u2068\"\n",
    "                               u\"\\u2067\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = weirdPatterns.sub(r'', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization & Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stemming(text):\n",
    "    stemmer = nltk.ISRIStemmer()\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    #stemming\n",
    "    word_list = [stemmer.stem(w) for w in  word_list]\n",
    "    return ' '.join(word_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ali\n",
      "[nltk_data]     Abdallah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tweet, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     12\u001b[0m     cleaned_tweet \u001b[38;5;241m=\u001b[39m clean_text(tweet)\n\u001b[1;32m---> 13\u001b[0m     stemmed_tweet \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_and_stemming\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_tweet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     df\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTWEET\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m stemmed_tweet\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Handle non-string values (e.g., empty strings or other types)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[108], line 3\u001b[0m, in \u001b[0;36mtokenize_and_stemming\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_and_stemming\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     stemmer \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mISRIStemmer()\n\u001b[1;32m----> 3\u001b[0m     word_list \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#stemming\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     word_list \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m  word_list]\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1328\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1460\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[0;32m   1430\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1433\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1394\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1396\u001b[0m \n\u001b[0;32m   1397\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[0;32m   1399\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Assuming df is your DataFrame with 'TWEET' column\n",
    "nltk.download('punkt')  # Download the punkt tokenizer if not already downloaded\n",
    "\n",
    "for i in range(len(df)):\n",
    "    tweet = df.at[i, 'TWEET']\n",
    "    \n",
    "    # Check if tweet is a string\n",
    "    if isinstance(tweet, str):\n",
    "        cleaned_tweet = clean_text(tweet)\n",
    "        stemmed_tweet = tokenize_and_stemming(cleaned_tweet)\n",
    "        df.at[i, 'TWEET'] = stemmed_tweet\n",
    "    else:\n",
    "        # Handle non-string values (e.g., empty strings or other types)\n",
    "        df.at[i, 'TWEET'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
