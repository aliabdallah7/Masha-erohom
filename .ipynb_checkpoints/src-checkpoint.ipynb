{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10065 entries, 0 to 10064\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   ID      10065 non-null  int64 \n",
      " 1    TWEET  10064 non-null  object\n",
      " 2    LABEL  10065 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 236.0+ KB\n",
      "None\n",
      "\n",
      "First few rows of the dataset:\n",
      "   ID                                              TWEET    LABEL\n",
      "0   1            Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ø§Ù„Ø¬Ø§ÙŠÙ‡ Ù‡ÙƒÙˆÙ† Ù„Ø³Ù‡ Ù Ø§Ù„ÙƒÙ„ÙŠÙ‡ ..     none\n",
      "1   2  Ø¹Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ù‡ ÙˆØµÙ„ Ù„93.7 % Ù…Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙŠØ¹Ù†ÙŠ...    anger\n",
      "2   3                         ÙƒØªÙ†Ø§ Ù†ÙŠÙ„Ù‡ Ù Ø­Ø¸Ù†Ø§ Ø§Ù„Ù‡Ø¨Ø§Ø¨ xD  sadness\n",
      "3   4  Ø¬Ù…ÙŠØ¹Ù†Ø§ Ù†Ø±ÙŠØ¯ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù‡Ø¯Ø§ÙÙ†Ø§ Ù„ÙƒÙ† ØªÙˆÙ†Ø³ ØªØ§Ù„Ù‚Øª ÙÙŠ Ø­Ø±...      joy\n",
      "4   5  Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ù†Ø¸Ø§Ù…Ù‡Ø§ Ù…Ø®ØªÙ„Ù .. ÙˆÙ…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù…ÙˆÙ†Ø¯ÙŠØ§Ù„ ...     none\n",
      "\n",
      "Missing values:\n",
      "ID        0\n",
      " TWEET    1\n",
      " LABEL    0\n",
      "dtype: int64\n",
      "##################\n",
      "ID        0\n",
      " TWEET    0\n",
      " LABEL    0\n",
      "dtype: int64\n",
      "Index(['ID', ' TWEET', ' LABEL'], dtype='object')\n",
      "Index(['ID', 'TWEET', 'LABEL'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset (assuming it's in CSV format)\n",
    "df = pd.read_csv('Emotional-Tone-Dataset.csv')\n",
    "\n",
    "# Explore the dataset\n",
    "print(\"Dataset information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove null values\n",
    "df = df.dropna()\n",
    "print(\"##################\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Rename Columns\n",
    "print(df.columns)\n",
    "df.rename(columns={' ID': 'ID', ' TWEET': 'TWEET', ' LABEL': 'LABEL'}, inplace=True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TWEET</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ø§Ù„Ø¬Ø§ÙŠÙ‡ Ù‡ÙƒÙˆÙ† Ù„Ø³Ù‡ Ù Ø§Ù„ÙƒÙ„ÙŠÙ‡ ..</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ø¹Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ù‡ ÙˆØµÙ„ Ù„93.7 % Ù…Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙŠØ¹Ù†ÙŠ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ÙƒØªÙ†Ø§ Ù†ÙŠÙ„Ù‡ Ù Ø­Ø¸Ù†Ø§ Ø§Ù„Ù‡Ø¨Ø§Ø¨ xD</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Ø¬Ù…ÙŠØ¹Ù†Ø§ Ù†Ø±ÙŠØ¯ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù‡Ø¯Ø§ÙÙ†Ø§ Ù„ÙƒÙ† ØªÙˆÙ†Ø³ ØªØ§Ù„Ù‚Øª ÙÙŠ Ø­Ø±...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ù†Ø¸Ø§Ù…Ù‡Ø§ Ù…Ø®ØªÙ„Ù .. ÙˆÙ…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù…ÙˆÙ†Ø¯ÙŠØ§Ù„ ...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10060</th>\n",
       "      <td>10061</td>\n",
       "      <td>2222: ÙŠÙ„Ø§ ÙŠØ§ Ø¬Ù…Ø§Ø¹Ù‡ Ø­ÙÙ„Ù‡ Ø¹Ù…Ø±Ùˆ Ø¯ÙŠØ§Ø¨ Ø®Ù„ØµØª Ù†Ø±ÙŠØ­ Ø´Ùˆ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10061</th>\n",
       "      <td>10062</td>\n",
       "      <td>Mohamed5: Ø§ÙŠÙŠÙŠÙŠÙ‡ Ø¯Ø§Ø§Ø§ ğŸ˜²ğŸ˜² Ø§ÙˆØ²ÙŠÙŠÙŠÙ„â¤</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10062</th>\n",
       "      <td>10063</td>\n",
       "      <td>Ø¹Ù…Ù„ØªÙ„Ù‡Ø§ Ø±ÙŠØªÙˆÙŠØª Ø¨Ù…Ù†Ø§Ø³Ø¨Ù‡ Ø³Ø§Ø±Ù‡ Ø¨ØªØ§Ø¹Ù‡ Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ ğŸ˜ƒ</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10063</th>\n",
       "      <td>10064</td>\n",
       "      <td>ÙˆØ¹Ù„ÙŠÙƒ Ù‚Ø¨Ù„Ù†Ø§ ÙŠØ§Ù†Ø¬Ù… Ø§Ù„Ù†Ø¬ÙˆÙ… ÙŠØ§Ø¹Ù†Ø¯Ù„ÙŠØ¨ Ø§Ù„Ø­Ø¨ ÙˆØ§Ù„Ø§Ø­Ø³Ø§Ø³</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10064</th>\n",
       "      <td>10065</td>\n",
       "      <td>AlHamad ÙŠØ·Ù„Ø¹ Ù†Ù†Ù‡Ù… ÙƒÙ„ Ø´ÙŠ Ø³ÙŠØ¡ ÙˆÙˆØ¶ÙŠØ¹ ÙƒÙ„ Ø®Ø³Ø§Ø³Ù‡ Ø§Ù„Ø¹...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10064 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                              TWEET     LABEL\n",
       "0          1            Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ø§Ù„Ø¬Ø§ÙŠÙ‡ Ù‡ÙƒÙˆÙ† Ù„Ø³Ù‡ Ù Ø§Ù„ÙƒÙ„ÙŠÙ‡ ..      none\n",
       "1          2  Ø¹Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ù‡ ÙˆØµÙ„ Ù„93.7 % Ù…Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙŠØ¹Ù†ÙŠ...     anger\n",
       "2          3                         ÙƒØªÙ†Ø§ Ù†ÙŠÙ„Ù‡ Ù Ø­Ø¸Ù†Ø§ Ø§Ù„Ù‡Ø¨Ø§Ø¨ xD   sadness\n",
       "3          4  Ø¬Ù…ÙŠØ¹Ù†Ø§ Ù†Ø±ÙŠØ¯ ØªØ­Ù‚ÙŠÙ‚ Ø§Ù‡Ø¯Ø§ÙÙ†Ø§ Ù„ÙƒÙ† ØªÙˆÙ†Ø³ ØªØ§Ù„Ù‚Øª ÙÙŠ Ø­Ø±...       joy\n",
       "4          5  Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ Ù†Ø¸Ø§Ù…Ù‡Ø§ Ù…Ø®ØªÙ„Ù .. ÙˆÙ…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù…ÙˆÙ†Ø¯ÙŠØ§Ù„ ...      none\n",
       "...      ...                                                ...       ...\n",
       "10060  10061  2222: ÙŠÙ„Ø§ ÙŠØ§ Ø¬Ù…Ø§Ø¹Ù‡ Ø­ÙÙ„Ù‡ Ø¹Ù…Ø±Ùˆ Ø¯ÙŠØ§Ø¨ Ø®Ù„ØµØª Ù†Ø±ÙŠØ­ Ø´Ùˆ...   sadness\n",
       "10061  10062                  Mohamed5: Ø§ÙŠÙŠÙŠÙŠÙ‡ Ø¯Ø§Ø§Ø§ ğŸ˜²ğŸ˜² Ø§ÙˆØ²ÙŠÙŠÙŠÙ„â¤  surprise\n",
       "10062  10063    Ø¹Ù…Ù„ØªÙ„Ù‡Ø§ Ø±ÙŠØªÙˆÙŠØª Ø¨Ù…Ù†Ø§Ø³Ø¨Ù‡ Ø³Ø§Ø±Ù‡ Ø¨ØªØ§Ø¹Ù‡ Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯ ğŸ˜ƒ      none\n",
       "10063  10064    ÙˆØ¹Ù„ÙŠÙƒ Ù‚Ø¨Ù„Ù†Ø§ ÙŠØ§Ù†Ø¬Ù… Ø§Ù„Ù†Ø¬ÙˆÙ… ÙŠØ§Ø¹Ù†Ø¯Ù„ÙŠØ¨ Ø§Ù„Ø­Ø¨ ÙˆØ§Ù„Ø§Ø­Ø³Ø§Ø³       joy\n",
       "10064  10065  AlHamad ÙŠØ·Ù„Ø¹ Ù†Ù†Ù‡Ù… ÙƒÙ„ Ø´ÙŠ Ø³ÙŠØ¡ ÙˆÙˆØ¶ÙŠØ¹ ÙƒÙ„ Ø®Ø³Ø§Ø³Ù‡ Ø§Ù„Ø¹...     anger\n",
       "\n",
       "[10064 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from stopwords import stop_words_ar\n",
    "\n",
    "############### initial variables and lists ##################\n",
    "\n",
    "arabic_punctuations = '''`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:\"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             Ù‘    | # Tashdid\n",
    "                             Ù    | # Fatha\n",
    "                             Ù‹    | # Tanwin Fath\n",
    "                             Ù    | # Damma\n",
    "                             ÙŒ    | # Tanwin Damm\n",
    "                             Ù    | # Kasra\n",
    "                             Ù    | # Tanwin Kasr\n",
    "                             Ù’    | # Sukun\n",
    "                             Ù€     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "arabic_handles = \\\n",
    "    [\n",
    "        ('Ù‡', ['Ø©']),\\\n",
    "        ('ÙŠ', ['Ù‰']),\\\n",
    "        ('Ø§', ['Ø£','Ø¥','Ø¢']),\\\n",
    "        # ('Ùˆ', ['Ø¤']),\\\n",
    "        ('', ['Ù‘','â€˜','ÙŒ','Ù','Ù‹','Ù','Ù','Ù','Ù€','â€™','Ù’','~'])\n",
    "    ]\n",
    "\n",
    "############### functions ##################\n",
    "def clean_text(text):\n",
    "    \"\"\" \n",
    "    It includes these functions:\n",
    "        1-remove_emails\n",
    "        2-remove_URLs\n",
    "        3-remove_mentions\n",
    "        4-hashtags_to_words\n",
    "        5-remove_punctuationsb\n",
    "        6-normalize_arabic\n",
    "        7-remove_diacritics\n",
    "        8-remove_repeating_char\n",
    "        9- remove newlines\n",
    "        10-remove_stop_words\n",
    "        11-remove_emojis\n",
    "        12-remove_english_characters\n",
    "        13-remove_digits\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return ''  # Return an empty string for non-string values\n",
    "    \n",
    "    text=remove_emails(text)\n",
    "    text=remove_URLs(text)\n",
    "    text=remove_mentions(text)\n",
    "    text= hashtags_to_words(text)\n",
    "    text=remove_punctuations(text)\n",
    "    text= remove_newlines(text)\n",
    "    text=normalize_arabic(text)\n",
    "    text=remove_diacritics(text)\n",
    "    text=remove_stop_words(text)\n",
    "    text=replace_emojis(text)\n",
    "    text=remove_english_characters(text)\n",
    "    text=remove_digits(text)\n",
    "    #text=remove_repeating_char(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    \"\"\" normalize the arabic character  .\"\"\"\n",
    "    text = re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
    "    text = re.sub(\"Ú¯\", \"Ùƒ\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_digits(word_list):\n",
    "    # Remove digits\n",
    "    filtered_words = ''.join([w for w in word_list if not w.isdigit()])\n",
    "    return filtered_words\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    \"\"\" remove the `arabic diacritics` from the `text` .\"\"\"\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    \"\"\" remove the `punctuations` from the `text` .\"\"\"\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    \"\"\" remove the `repeating character` from the `text` .\"\"\"\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "def remove_newlines(text):\n",
    "  text = re.sub('\\n',\" . \",text)\n",
    "  return text\n",
    "\n",
    "def read_stop_words():\n",
    "    \"\"\" read the `stopwords` \"\"\"\n",
    "    stop_words = stop_words_ar.split('\\n')\n",
    "    #unify arabic letters\n",
    "    for key, arr in arabic_handles:\n",
    "        for a in arr:\n",
    "            stop_words = [word.replace(a, key) for word in stop_words] \n",
    "    return stop_words\n",
    "\n",
    "def remove_english_characters(text):\n",
    "    # Define a regular expression pattern to match English characters\n",
    "    english_pattern = re.compile(\"[a-zA-Z]\")\n",
    "\n",
    "    # Use sub to replace English characters with an empty string\n",
    "    cleaned_text = english_pattern.sub('', text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    \"\"\" remove the `list of Arabic stopwords` from the `text` .\"\"\"\n",
    "    stop_words = read_stop_words()\n",
    "    lines = text.splitlines()\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        if line.strip():  #if not empty\n",
    "            words = line.split()\n",
    "            for w in range(0, len(words)):\n",
    "                word = words[w]\n",
    "                if word in stop_words:\n",
    "                    words[w] = \"\"\n",
    "            line = \" \".join(words)\n",
    "            line = line.replace(\"  \",\" \")\n",
    "            new_lines.append(line)\n",
    "    return '\\n'.join(new_lines)\n",
    "\n",
    "def remove_URLs(text): \n",
    "    \"\"\" remove the `URLs` from the `text` .\"\"\"\n",
    "    text =re.sub(r\"(?:http?\\://|https?\\://|www)\\S+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_emails(text):\n",
    "    \"\"\" remove the `emails` from the `text` .\"\"\"\n",
    "    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+',\"\",text)\n",
    "    return text\n",
    "\n",
    "def remove_mentions(text):\n",
    "    \"\"\" remove the `mentions` from the `text` .\"\"\"\n",
    "    text = re.sub(r\"@([A-Za-z0-9_-]+)\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\" remove the `hashtags` from the `text` .\"\"\"\n",
    "    text = re.sub(r'#\\w+ ?', \"\", text)\n",
    "    return text\n",
    "\n",
    "def hashtags_to_words(text):\n",
    "    \"\"\" convert any `hashtags` to  `words` .\"\"\"\n",
    "    text = re.sub(r'#', \"\", text)\n",
    "    text = re.sub(r\"_\", \"  \", text)\n",
    "    return text\n",
    "\n",
    "def replace_emojis(text): \n",
    "    text = re.sub(r'<3|< 3|â¤ï¸|ğŸ’–|ğŸ˜|ğŸ’•|ğŸ˜˜|ğŸ¥°|ğŸ’•|ğŸ’|ğŸ’—|ğŸ’œ|ğŸ’™|ğŸ–¤|ğŸ’š|ğŸ’›|ğŸ¤|â¤',\n",
    "                  ' Ù‚Ù„Ø¨ ',\n",
    "                  text) \n",
    "\n",
    "    text = re.sub(r':P|:-P|ğŸ˜‚|ğŸ¤£',\n",
    "                  ' Ø¶Ø­Ùƒ ',\n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'[â˜ºğŸ˜ŒğŸ˜ğŸ˜ƒğŸ˜„ğŸ˜†ğŸ˜ŠğŸ˜¸ğŸ˜ºğŸ˜ŠğŸ˜€ğŸ˜‹â˜ºï¸ğŸ™‚ğŸ’ƒ]',\n",
    "                  ' Ø³Ø¹Ø§Ø¯Ø© ',\n",
    "                  text)\n",
    "    \n",
    "    \n",
    "    text = re.sub(r':D',\n",
    "                  ' Ø³Ø¹Ø§Ø¯Ø© ',\n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'[ğŸ˜¥ğŸ˜£ğŸ˜“ğŸ˜”ğŸ˜•â˜¹ï¸ğŸ™ğŸ˜–ğŸ˜ğŸ˜ŸğŸ˜¢ğŸ˜­ğŸ˜©ğŸ˜¿ğŸ˜«ğŸ˜©ğŸ’”]',\n",
    "                  ' Ø­Ø²Ù†  ',\n",
    "                  text)\n",
    "    text = re.sub(r'(::|\\)-:)',\n",
    "                  '  Ø­Ø²Ù†  ', \n",
    "                  text)\n",
    "    text = re.sub(r'(:,\\(|:\\'\\(|:\"\\()',\n",
    "                  ' Ø­Ø²Ù† ', \n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'[ğŸ˜¨ğŸ˜±ğŸ˜µ]',\n",
    "                  ' Ù…ÙØ§Ø¬Ø£Ø© ', \n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'[ğŸ˜³ğŸ˜…ğŸ™ˆ]',\n",
    "                  ' Ù…Ø­Ø±Ø¬ ', \n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'[ğŸ˜¤ğŸ˜ ğŸ˜¡ğŸ¤¬ğŸ‘¿]',\n",
    "                  ' ØºØ¶Ø¨ ', \n",
    "                  text)\n",
    "\n",
    "    text = re.sub(r'[ğŸ˜‘ğŸ˜’ğŸ™„ğŸ˜ğŸ˜¶]',\n",
    "                  ' Ù…Ù„Ù„ ', \n",
    "                  text)\n",
    "\n",
    "    text = re.sub('[\\U0001F600-\\U0001FFFF]',\" \", text)\n",
    "    text = re.sub('[\\U0001F300-\\U0001F5FF]',\" \", text)\n",
    "    text = re.sub('[\\U0001F680-\\U0001F6FF]',\" \", text)\n",
    "    text = re.sub('[\"\\U0001F1E0-\\U0001F1FF]',\" \", text)\n",
    "\n",
    "    \n",
    "    weirdPatterns = re.compile(\"[\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u'\\U00010000-\\U0010ffff'\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               u\"\\u2069\"\n",
    "                               u\"\\u2066\"\n",
    "                               u\"\\u200c\"\n",
    "                               u\"\\u2068\"\n",
    "                               u\"\\u2067\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = weirdPatterns.sub(r'', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization & Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stemming(text):\n",
    "    stemmer = nltk.ISRIStemmer()\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    #stemming\n",
    "    word_list = [stemmer.stem(w) for w in  word_list]\n",
    "    return ' '.join(word_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ali\n",
      "[nltk_data]     Abdallah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "8619",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 8619",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Download the punkt tokenizer if not already downloaded\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[1;32m----> 8\u001b[0m     tweet \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTWEET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Check if tweet is a string\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tweet, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\indexing.py:2431\u001b[0m, in \u001b[0;36m_AtIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2428\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid call for scalar access (getting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mloc[key]\n\u001b[1;32m-> 2431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\indexing.py:2382\u001b[0m, in \u001b[0;36m_ScalarAccessIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2379\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid call for scalar access (getting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2381\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_key(key)\n\u001b[1;32m-> 2382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\frame.py:3924\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   3918\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[0;32m   3920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   3921\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[0;32m   3922\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[0;32m   3923\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[1;32m-> 3924\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[row]\n\u001b[0;32m   3927\u001b[0m \u001b[38;5;66;03m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[0;32m   3928\u001b[0m \u001b[38;5;66;03m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Public\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 8619"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Assuming df is your DataFrame with 'TWEET' column\n",
    "#nltk.download('punkt')  # Download the punkt tokenizer if not already downloaded\n",
    "\n",
    "for i in range(len(df)):\n",
    "    tweet = df.at[i, 'TWEET']\n",
    "    \n",
    "    # Check if tweet is a string\n",
    "    if isinstance(tweet, str):\n",
    "        cleaned_tweet = clean_text(tweet)\n",
    "        stemmed_tweet = tokenize_and_stemming(cleaned_tweet)\n",
    "        df.at[i, 'TWEET'] = stemmed_tweet\n",
    "    else:\n",
    "        # Handle non-string values (e.g., empty strings or other types)\n",
    "        df.at[i, 'TWEET'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
